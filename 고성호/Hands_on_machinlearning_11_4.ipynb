{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hands_on_machinlearning_11.4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPLAhEfDlLAx"
      },
      "source": [
        "#11.4 규제를 사용한 과대적합 피하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mus_whd8lfo5"
      },
      "source": [
        "###11.4.1 $l_1$ 규제와 $l_2$ 규제\n",
        "\n",
        "* 오버피팅(overfitting)\n",
        "  * 학습 데이터에 대해서는 과도하게 학습이 되어 좋은 예측력을 보이지만, 새로운  데이터에 대해서는 예측력이 떨어지는 것\n",
        "\n",
        "  <img src = 'https://drive.google.com/uc?export=view&id=1RItK50oeIqqRX72PkN-UmgYl6thAcp_Q' align = 'middle'>\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNRGUKM7bFbj"
      },
      "source": [
        "* 기본 아이디어\n",
        "  <img src='https://drive.google.com/uc?export=view&id=10LLxS6nD8KVJkhjBUIaUK2WuTPqL0H03' width=700>\n",
        "\n",
        "  * parameter 갱신은 Cost function을 최소화 하는 것을 목표로 하여 진행된다.\n",
        "  * 만약 cost function이  $\\displaystyle\\sum(y_i - \\hat y_i )^2$라고 했을 때, weight가 $ \\beta_1, \\beta_2, \\beta_3, \\beta_4 $ 로 4개가 존재하고, bias가 $\\beta_0$ 라고 가정해보자.\n",
        "  * 현재 규제가 없는 cost function을 사용할 때 우측 그림과 같이 overfitting 되고 있다.\n",
        "  * 이를 좌측 그림과 같이 더 general한 모델로 만들기 위해서는 weight들에 규제를 가해야 할 필요성이 있다.\n",
        "  * 가장 극단적인 규제는 몇 개의 weight를 아예 0으로 만드는 방법이다. \n",
        "  * $min_\\beta \\sum(y_i - \\hat y_i)^2 + 5000 \\beta_3^2 + 5000 \\beta_4 ^2$ \n",
        "  * $\\beta_3, \\beta4$에 매우 큰 수준의 규제로 5000을 곱해주었다. cost function을 최소화 하기 위해서는 $\\beta_3$와 $\\beta_4$가 0에 가까워져야 할 것이다.\n",
        "  * 결국 우측 그림에서 좌측 그림으로 overfitting 문제가 해결되었다.\n",
        "  * 위 예시는 이해를 돕기 위한 극단적인 상황을 가정한 것이고, 실제로는 저렇게 큰 수를 곱해서 weight를 완전히 죽이지는 않는다.\n",
        "  * 이렇게 weight에 특정 값을 곱해서 제약을 가함으로써 overfitting을 해결하는 것이 L1, L2 규제의 기본 아이디어이다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8wU0fgTVsU"
      },
      "source": [
        "* norm이란?\n",
        "  * $L^x = || p-q ||_x = (\\sum |p_i-q_i|^x)^\\frac{1}{x})$\n",
        "  * 두 벡터간의 크기 또는 길이를 뜻한다.\n",
        "  * x의 값에 따라 $l^1 ~~norm, l^2 ~~norm ... l^\\infty$ 로 표기되고, x의 값의 범위는 1보다 크거나 같은 실수이다.\n",
        "  * 벡터 x의 $norm$은 원점에서 x까지의 거리를 나타낸다.\n",
        "\n",
        " <img src = 'https://drive.google.com/uc?export=view&id=1Ty1yBWyC4vah1jUZVSO6VKJiqoVJpJmQ' width = 300>\n",
        "\n",
        "  * $L^1 norm$\n",
        "    * $L^1 = \\displaystyle \\sum_{i=1}^{n} |p-q| = |  p_1-q_1|+|p_2-q_2|+|p_3-q_3|+ ... +|p_n-q_n|$\n",
        "    * 맨하탄 거리(Manhattan Distance)라고도 한다.\n",
        "\n",
        "  * $L^2 norm$\n",
        "    * $L^1 =\\displaystyle(\\sum_{i=1}^{n} |p-q|^2) ^\\frac{1}{2} = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+( p_3-q_3)^2+ ... +(p_n-q_n)^2}$\n",
        "    * 유클리디안 거리(Euclidean norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSezZy02j7ta"
      },
      "source": [
        "* L1 규제(L1 Regularization)\n",
        " <img src = 'https://drive.google.com/uc?export=view&id=1dB0P0H9MmyOnlt1oWUzAJ4AYw3tcGlP4'>\n",
        "\n",
        " * 앞에 붙는 $\\frac{1}{n}$은 case에 따라 다르게 적용함.\n",
        " * $\\lambda$의 경우 parameter로서 keras 사용시 임의의 값을 부여할 수 있음.\n",
        "\n",
        "\n",
        "* L1 규제가 적용된 cost function 미분\n",
        "  * L1 규제가 적용된 cost function을 더욱 간단하게 표현해 보았다. $Cost = C_0 + \\frac{\\lambda}{n} \\sum |w|$\n",
        "  * 이 식을 w에 대해 편미분하면 다음과 같다. $w → w - \\frac{\\eta \\lambda}{n}sgn(w) - \\eta \\frac{\\partial C_0}{\\partial w}$\n",
        "  * 위 식에서 weight는 전 step의 weight의 크기와 상관 없이 weight의 부호에 따라 상수 값을 빼주주는 방식으로 갱신된다.\n",
        "\n",
        "\n",
        "* L2 규제(L2 Regularization)\n",
        "<img src = 'https://drive.google.com/uc?export=view&id=1sTI6kLMuMfZ7-Es13jf7we46C4t7JT5Z'>\n",
        "\n",
        "  * L2 규제가 적용된 cost function 미분\n",
        "  * L2 규제가 적용된 cost function을 더욱 간단하게 표현해 보았다. $Cost = C_0 + \\frac{\\lambda}{2n} \\sum w^2$\n",
        "  * 이 식을 w에 대해 편미분 하면 다음과 같다. \n",
        "  * $w → w - \\frac{\\eta \\lambda}{n} - \\eta \\frac{\\partial C_0}{\\partial w}$\n",
        "  * = $(1 - \\frac{\\eta \\lambda}{n})w - \\eta \\frac{\\partial C_0}{\\partial w}$\n",
        "  * 위 식에서 weight는 전 setp의 weight에 $(1 - \\frac{\\eta \\lambda}{n})$를 곱해주며 갱신된다.\n",
        "\n",
        "\n",
        "* 결론 : L1 규제와 L2 규제 중 무엇을 사용해야 하는가?\n",
        "  * L1 규제는 규제의 크기($\\lambda$)가 커질수록 가중치가 0에 가까워지기 때문에 underfitting될 가능성이 크다.\n",
        "  * 하지만, L2 규제는 규제의 크기($\\lambda$)가 커져도 가중치가 0에 가까워지는 정도가 덜 하기 때문에 underfitting이 잘 되지 않는다.\n",
        "  * 결론적으로는 L1 규제보다 L2 규제를 더 많이 사용한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhUKKmN_nOCL"
      },
      "source": [
        "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
        "from tensorflow import keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rus5JQbbndd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2b5a1e-3195-48bd-a2cb-ce71907105e1"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = load_data()\n",
        "X_train = X_train/255.\n",
        "X_test = X_test/255."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9M9qpyTMkob",
        "outputId": "caf7675f-9128-42df-d8bb-ca800ff352f2"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj-DXQkRojmY"
      },
      "source": [
        "class_names=['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXtjmDO1zwLR"
      },
      "source": [
        "layer=keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal',\n",
        "                         kernel_regularizer=keras.regularizers.l2(0.01))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abK0_Ju_z-1W"
      },
      "source": [
        "#Code refactoring\n",
        "from functools import partial\n",
        "\n",
        "RegularizedDense=partial(keras.layers.Dense,\n",
        "                         activation='elu',\n",
        "                         kernel_initializer='he_normal',\n",
        "                         kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "model=keras.models.Sequential([\n",
        "                               keras.layers.Flatten(input_shape=[28, 28]),\n",
        "                               RegularizedDense(300),\n",
        "                               RegularizedDense(100),\n",
        "                               RegularizedDense(10, activation='softmax', kernel_initializer='glorot_uniform')\n",
        "])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwZ7122soq2a"
      },
      "source": [
        "##11.4.2 드롭아웃(Dropout)\n",
        " <img src = 'https://drive.google.com/uc?export=view&id=1hBxujo4dufY_B31UbTkIrtEWXzgoPiJz' width=600>\n",
        "\n",
        "* 방법\n",
        "  * 매 training step에서 각 뉴런은 임시적으로 드롭아웃 될 확률은 p이다(예를 들어 p가 0.5일 경우 전체 뉴런 중 50%의 뉴런이 해당 step 훈련에서 제외된다).\n",
        "  * 드롭아웃 되는 뉴런들은 해당 step에서 훈련에서 제외된다.\n",
        "  * 훈련이 끝난 후 최종 모델(test 혹은 상용화를 위한 모델)에서는 dropout을 적용하지 않고 모든 뉴런을 다 사용한다. 단, 각 뉴런은 훈련 때 드롭아웃으로 인해 더 적은 입력 뉴런과 연결되었기 때문에 훈련이 끝난 후 모든 뉴런을 다 사용할 때에는 각 가중치에 **보존확률 = (1-p)**를 곱해줘야 한다.\n",
        "\n",
        "* 드롭 아웃이 작동하는 원리\n",
        "  * Co-adaptation 방지 : 신경망 학습 중 어느 시점에서 같은 층의 두 개 이상의 뉴런의 입력 및 출력 연결 강도가 같아지만, 학습을 계속 진행해도 해당 뉴런들은 같은 일을 수행하게 되어 불필요한 중복이 생기게 된다. 드롭 아웃은 이러한 Co-adaptation을 방지하는 방식으로 신경망 성능을 높인다.\n",
        "  * Ensemble : step별로 확률에 따라 뉴런들이 드롭 아웃 되면서, 더 작은 네트워크가 형성되고 그 네트워크로 훈련을 진행하게 된다. 그리고 그 다음 step엔 이전에 훈련되었던 네트워크를 반영해 뉴런을 드롭 아웃 후 다른 네트워크로 훈련을 진행한다. 결국 매 epoch마다 다른 모델을 만들고 이를 다음 epoch에 반영하여 모델을 만들기 때문에 Ensemble효과가 발생한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFUzA3LDppBy"
      },
      "source": [
        "model_normal=keras.models.Sequential([\n",
        "                               keras.layers.Flatten(input_shape=[28,28]),\n",
        "                               keras.layers.Dense(300,activation='elu',kernel_initializer='he_normal'),\n",
        "                               keras.layers.Dense(100,activation='elu',kernel_initializer='he_normal'),\n",
        "                               keras.layers.Dense(10,activation='softmax')\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpGyJQ53p2HK",
        "outputId": "f266b711-fd35-400e-9e4b-733fe0a1554f"
      },
      "source": [
        "model_normal.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB6B3a7wpz67"
      },
      "source": [
        "model_normal.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['acc'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5VgwhVpqRBg",
        "outputId": "4a3d3644-f5eb-4f13-a7f0-40011351ded1"
      },
      "source": [
        "hist_2=model_normal.fit(X_train, y_train,\n",
        "                         epochs=25,\n",
        "                         validation_split=0.3,\n",
        "                         batch_size=128)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "329/329 [==============================] - 3s 8ms/step - loss: 0.5372 - acc: 0.8083 - val_loss: 0.4289 - val_acc: 0.8437\n",
            "Epoch 2/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.3987 - acc: 0.8541 - val_loss: 0.4090 - val_acc: 0.8504\n",
            "Epoch 3/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.3547 - acc: 0.8704 - val_loss: 0.3537 - val_acc: 0.8719\n",
            "Epoch 4/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.3283 - acc: 0.8776 - val_loss: 0.3600 - val_acc: 0.8696\n",
            "Epoch 5/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.3117 - acc: 0.8856 - val_loss: 0.3373 - val_acc: 0.8764\n",
            "Epoch 6/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.2902 - acc: 0.8914 - val_loss: 0.3347 - val_acc: 0.8806\n",
            "Epoch 7/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.2790 - acc: 0.8962 - val_loss: 0.4177 - val_acc: 0.8520\n",
            "Epoch 8/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.2672 - acc: 0.8999 - val_loss: 0.3277 - val_acc: 0.8801\n",
            "Epoch 9/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.2536 - acc: 0.9040 - val_loss: 0.3089 - val_acc: 0.8888\n",
            "Epoch 10/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.2427 - acc: 0.9083 - val_loss: 0.3347 - val_acc: 0.8811\n",
            "Epoch 11/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.2313 - acc: 0.9124 - val_loss: 0.3458 - val_acc: 0.8792\n",
            "Epoch 12/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.2268 - acc: 0.9133 - val_loss: 0.3456 - val_acc: 0.8803\n",
            "Epoch 13/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.2139 - acc: 0.9194 - val_loss: 0.3262 - val_acc: 0.8881\n",
            "Epoch 14/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.2094 - acc: 0.9215 - val_loss: 0.3405 - val_acc: 0.8858\n",
            "Epoch 15/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1995 - acc: 0.9244 - val_loss: 0.3208 - val_acc: 0.8879\n",
            "Epoch 16/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1927 - acc: 0.9268 - val_loss: 0.3134 - val_acc: 0.8908\n",
            "Epoch 17/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1884 - acc: 0.9285 - val_loss: 0.3099 - val_acc: 0.8933\n",
            "Epoch 18/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1813 - acc: 0.9307 - val_loss: 0.3285 - val_acc: 0.8836\n",
            "Epoch 19/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1739 - acc: 0.9334 - val_loss: 0.3355 - val_acc: 0.8899\n",
            "Epoch 20/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1667 - acc: 0.9365 - val_loss: 0.3508 - val_acc: 0.8867\n",
            "Epoch 21/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1669 - acc: 0.9355 - val_loss: 0.3485 - val_acc: 0.8917\n",
            "Epoch 22/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1574 - acc: 0.9407 - val_loss: 0.3423 - val_acc: 0.8892\n",
            "Epoch 23/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1511 - acc: 0.9428 - val_loss: 0.3645 - val_acc: 0.8887\n",
            "Epoch 24/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1439 - acc: 0.9451 - val_loss: 0.3417 - val_acc: 0.8933\n",
            "Epoch 25/25\n",
            "329/329 [==============================] - 2s 7ms/step - loss: 0.1384 - acc: 0.9476 - val_loss: 0.3751 - val_acc: 0.8854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URtD27xNykAM"
      },
      "source": [
        "model_dropout=keras.models.Sequential([\n",
        "                               keras.layers.Flatten(input_shape=[28,28]),\n",
        "                               keras.layers.Dropout(rate=0.2),\n",
        "                               keras.layers.Dense(300,activation='elu',kernel_initializer='he_normal'),\n",
        "                               keras.layers.Dropout(rate=0.2),\n",
        "                               keras.layers.Dense(100,activation='elu',kernel_initializer='he_normal'),\n",
        "                               keras.layers.Dropout(rate=0.2),\n",
        "                               keras.layers.Dense(10,activation='softmax')\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDFSC7mJo3vI",
        "outputId": "3d47b8f7-e329-43bb-922a-df3da58cc9a0"
      },
      "source": [
        "model_dropout.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkT1g429pMVx"
      },
      "source": [
        "model_dropout.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['acc'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVDMxVQnpC5m",
        "outputId": "23cbe427-f350-42b7-b2e5-01f0ab6f39df"
      },
      "source": [
        "hist_2=model_dropout.fit(X_train, y_train,\n",
        "                         epochs=25,\n",
        "                         validation_split=0.3,\n",
        "                         batch_size=128)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "329/329 [==============================] - 4s 10ms/step - loss: 0.6582 - acc: 0.7615 - val_loss: 0.4571 - val_acc: 0.8347\n",
            "Epoch 2/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.4922 - acc: 0.8202 - val_loss: 0.4034 - val_acc: 0.8502\n",
            "Epoch 3/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.4529 - acc: 0.8318 - val_loss: 0.3804 - val_acc: 0.8602\n",
            "Epoch 4/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.4303 - acc: 0.8392 - val_loss: 0.3797 - val_acc: 0.8603\n",
            "Epoch 5/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.4111 - acc: 0.8461 - val_loss: 0.3599 - val_acc: 0.8687\n",
            "Epoch 6/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3986 - acc: 0.8511 - val_loss: 0.3472 - val_acc: 0.8732\n",
            "Epoch 7/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3867 - acc: 0.8557 - val_loss: 0.3458 - val_acc: 0.8719\n",
            "Epoch 8/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.3774 - acc: 0.8585 - val_loss: 0.3430 - val_acc: 0.8731\n",
            "Epoch 9/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3672 - acc: 0.8612 - val_loss: 0.3314 - val_acc: 0.8793\n",
            "Epoch 10/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.3640 - acc: 0.8618 - val_loss: 0.3288 - val_acc: 0.8809\n",
            "Epoch 11/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3533 - acc: 0.8663 - val_loss: 0.3306 - val_acc: 0.8782\n",
            "Epoch 12/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3489 - acc: 0.8684 - val_loss: 0.3251 - val_acc: 0.8805\n",
            "Epoch 13/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3465 - acc: 0.8688 - val_loss: 0.3310 - val_acc: 0.8764\n",
            "Epoch 14/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3407 - acc: 0.8730 - val_loss: 0.3157 - val_acc: 0.8852\n",
            "Epoch 15/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3363 - acc: 0.8731 - val_loss: 0.3222 - val_acc: 0.8791\n",
            "Epoch 16/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3308 - acc: 0.8747 - val_loss: 0.3089 - val_acc: 0.8869\n",
            "Epoch 17/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3280 - acc: 0.8759 - val_loss: 0.3120 - val_acc: 0.8872\n",
            "Epoch 18/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3207 - acc: 0.8786 - val_loss: 0.3158 - val_acc: 0.8846\n",
            "Epoch 19/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.3211 - acc: 0.8778 - val_loss: 0.3107 - val_acc: 0.8867\n",
            "Epoch 20/25\n",
            "329/329 [==============================] - 3s 10ms/step - loss: 0.3141 - acc: 0.8781 - val_loss: 0.3037 - val_acc: 0.8920\n",
            "Epoch 21/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.3115 - acc: 0.8812 - val_loss: 0.3052 - val_acc: 0.8851\n",
            "Epoch 22/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.3100 - acc: 0.8824 - val_loss: 0.3057 - val_acc: 0.8879\n",
            "Epoch 23/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.3066 - acc: 0.8834 - val_loss: 0.3201 - val_acc: 0.8827\n",
            "Epoch 24/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.3044 - acc: 0.8848 - val_loss: 0.3003 - val_acc: 0.8926\n",
            "Epoch 25/25\n",
            "329/329 [==============================] - 3s 9ms/step - loss: 0.3013 - acc: 0.8841 - val_loss: 0.2920 - val_acc: 0.8926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9KiB_qDBDMt",
        "outputId": "fc2ca793-94ee-41ff-d6af-4575295ba83f"
      },
      "source": [
        "model_dropout.evaluate(X_test, y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3227 - acc: 0.8822\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.32266896963119507, 0.8822000026702881]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v8zKoXj1GfG"
      },
      "source": [
        "##11.4.3 몬테 카를로 드롭아웃(Monte-Carlo Dropout)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdkhO2ys_B8y"
      },
      "source": [
        "* 일반적인 드롭아웃과는 달리, 몬테 카를로 드롭아웃을 적용하게 되면, test과정에서도 드롭아웃을 적용하고, 여러 개의 예측 결과를 통해 더욱 '불확실성'이 줄어든 예측을 할 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHpk-PCjmKnH"
      },
      "source": [
        "import numpy as np\n",
        "y_probas = np.stack([model_dropout(X_test, training=True)\n",
        "                     for sample in range(100)])\n",
        "y_proba=y_probas.mean(axis=0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DunAW4Q4mjXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2380aa7-b6de-46f9-b404-54aab9bbdb7b"
      },
      "source": [
        "#MC 드롭아웃을 적용하지 않은 예측 결과\n",
        "np.round(model_dropout.predict(X_test[:1]), 2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUXMBpn0G_Y8",
        "outputId": "b948e22f-b54a-4d02-da41-eb3ebce98937"
      },
      "source": [
        "np.round(y_probas[:,:1],2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.07, 0.  , 0.72]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.42, 0.  , 0.53]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.95]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.07, 0.  , 0.89]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.14, 0.  , 0.82]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.89]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.25, 0.  , 0.68]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.9 ]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.07, 0.  , 0.87]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.14, 0.  , 0.62]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.27, 0.  , 0.72]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.06, 0.  , 0.92]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.96]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.07, 0.  , 0.9 ]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.01, 0.  , 0.89]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.91]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.96]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.02, 0.  , 0.95]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.09, 0.  , 0.66]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.45, 0.  , 0.54]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.4 , 0.  , 0.6 ]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.02, 0.  , 0.91]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.21, 0.  , 0.76]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.89]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.17, 0.  , 0.82]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.97]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.92]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.  , 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.07, 0.  , 0.89]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.94]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.46, 0.  , 0.53]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.15, 0.  , 0.83]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.95]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.89]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.05, 0.  , 0.89]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.44, 0.  , 0.13, 0.02, 0.41]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.39, 0.  , 0.  , 0.  , 0.61]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.65, 0.  , 0.03, 0.  , 0.32]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.07, 0.  , 0.86]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.01, 0.  , 0.96]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.92]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.35, 0.  , 0.  , 0.  , 0.65]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.09, 0.  , 0.85]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.02, 0.  , 0.76]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.85]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.02, 0.  , 0.93]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.11, 0.  , 0.88]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.02, 0.  , 0.93]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.01, 0.  , 0.66]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.  , 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.03, 0.  , 0.92]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.93]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.05, 0.  , 0.94]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.48, 0.  , 0.01, 0.  , 0.51]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.15, 0.  , 0.82]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.17, 0.  , 0.83]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.9 ]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.13, 0.  , 0.75]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.94]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.46, 0.  , 0.54]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.68, 0.  , 0.26, 0.  , 0.06]],\n",
              "\n",
              "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLTAavB1L3v9",
        "outputId": "a2afe123-aac4-4ce3-c94a-8ba049be5ae6"
      },
      "source": [
        "#axis=0 기준으로 평균값을 구함.\n",
        "np.round(y_proba[:1], 2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.08, 0.  , 0.87]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTQtoCYmNVEL",
        "outputId": "0e14acc4-7b9a-4952-99d8-87ad70b8fa39"
      },
      "source": [
        "y_std=y_probas.std(axis=0)\n",
        "np.round(y_std[:1], 2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.1 , 0.  , 0.17]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLqEjPWW8yb1"
      },
      "source": [
        "y_pred = np.argmax(y_proba, axis=1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPr_v-la8e7y",
        "outputId": "002ac212-c55c-47c8-9611-b5bbdec8421a"
      },
      "source": [
        "accuracy=np.sum(y_pred==y_test) / len(y_test)\n",
        "accuracy"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8824"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-QnyDaSNttm"
      },
      "source": [
        "##11.4.4 맥스-노름 규제(max-norm regularization)\n",
        "\n",
        "* 각 뉴런에 대한 입력의 연결 가중치 w가 다음과 같도록 제한한다($||w||_2 \\leq r$).\n",
        "* 맥스-노름 규제에서는 손실 함수에 규제 항을 추가하지 않는다.\n",
        "* 대신 매 훈련 스텝이 끝날 때마다 w의 norm의 계산하여 다음과 같이 스케일 조정을 시행한다. $w ← w \\frac{r}{||w||_2}$ \n",
        "*r은 하이퍼 파라미터로서 조절 가능하며, r을 줄이면 w에 더 작은 값이 곱해지기 때문에 가중치가 더욱 작아지게 되고 overfitting을 감소시키는데 도움이 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKPi1VCxzOxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e2096b-8d4f-47b9-876e-cefa4d18bbdd"
      },
      "source": [
        "keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal',\n",
        "                   kernel_constraint=keras.constraints.max_norm(1.))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.Dense at 0x7f0465e01750>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}